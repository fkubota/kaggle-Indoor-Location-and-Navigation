{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joined-burden",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- waypointをsite毎に保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compound-treasury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285dc1a\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "cmd = \"git rev-parse --short HEAD\"\n",
    "hash = subprocess.check_output(cmd.split()).strip().decode('utf-8')\n",
    "print(hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-cycling",
   "metadata": {},
   "source": [
    "# Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "arctic-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = '010'\n",
    "DIR_TRAIN = './../data_ignore/input/train/'\n",
    "DIR_TEST = './../data_ignore/input/test/'\n",
    "DIR_WIFI = './../data_ignore/input/wifi/'\n",
    "PATH_SUB = './../data_ignore/input/sample_submission.csv'\n",
    "PATH_99_SUB = './../data/input/floor_99per_acc_sub.csv'\n",
    "DIR_SAVE_IGNORE = f'./../data_ignore/nb/{NB}/'\n",
    "DIR_SAVE = f'./../data/nb/{NB}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-stupid",
   "metadata": {},
   "source": [
    "# Import everything I need:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latter-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import types\n",
    "import random\n",
    "import pickle\n",
    "import builtins\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from icecream import ic\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "# from tqdm import tqdm\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from glob import glob\n",
    "from loguru import logger\n",
    "from collections import OrderedDict\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-reducing",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subject-president",
   "metadata": {
    "executionInfo": {
     "elapsed": 4655,
     "status": "ok",
     "timestamp": 1616479151188,
     "user": {
      "displayName": "ユウキハヤカワ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjfXu9o7zmAYL24s4P8H7I0lvSgQUaSfrXi7qemvQ=s64",
      "userId": "00286900621093795459"
     },
     "user_tz": -540
    },
    "id": "7SxgGjwA5Rty"
   },
   "outputs": [],
   "source": [
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        # module imports\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield name, val\n",
    "\n",
    "            # functions / callables\n",
    "        if hasattr(val, '__call__'):\n",
    "            yield name, val\n",
    "\n",
    "\n",
    "def noglobal(f):\n",
    "    '''\n",
    "    ref: https://gist.github.com/raven38/4e4c3c7a179283c441f575d6e375510c\n",
    "    '''\n",
    "    return types.FunctionType(f.__code__,\n",
    "                              dict(imports()),\n",
    "                              f.__name__,\n",
    "                              f.__defaults__,\n",
    "                              f.__closure__\n",
    "                              )\n",
    "\n",
    "\n",
    "def comp_metric(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "registered-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReadData:\n",
    "    acce: np.ndarray\n",
    "    acce_uncali: np.ndarray\n",
    "    gyro: np.ndarray\n",
    "    gyro_uncali: np.ndarray\n",
    "    magn: np.ndarray\n",
    "    magn_uncali: np.ndarray\n",
    "    ahrs: np.ndarray\n",
    "    wifi: np.ndarray\n",
    "    ibeacon: np.ndarray\n",
    "    waypoint: np.ndarray\n",
    "\n",
    "\n",
    "def read_data_file(data_filename):\n",
    "    acce = []\n",
    "    acce_uncali = []\n",
    "    gyro = []\n",
    "    gyro_uncali = []\n",
    "    magn = []\n",
    "    magn_uncali = []\n",
    "    ahrs = []\n",
    "    wifi = []\n",
    "    ibeacon = []\n",
    "    waypoint = []\n",
    "\n",
    "    with open(data_filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line_data in lines:\n",
    "        line_data = line_data.strip()\n",
    "        if not line_data or line_data[0] == '#':\n",
    "            continue\n",
    "\n",
    "        line_data = line_data.split('\\t')\n",
    "\n",
    "        if line_data[1] == 'TYPE_ACCELEROMETER':\n",
    "            acce.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_ACCELEROMETER_UNCALIBRATED':\n",
    "            acce_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_GYROSCOPE':\n",
    "            gyro.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_GYROSCOPE_UNCALIBRATED':\n",
    "            gyro_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_MAGNETIC_FIELD':\n",
    "            magn.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_MAGNETIC_FIELD_UNCALIBRATED':\n",
    "            magn_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_ROTATION_VECTOR':\n",
    "            ahrs.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_WIFI':\n",
    "            sys_ts = line_data[0]\n",
    "            ssid = line_data[2]\n",
    "            bssid = line_data[3]\n",
    "            rssi = line_data[4]\n",
    "            lastseen_ts = line_data[6]\n",
    "            wifi_data = [sys_ts, ssid, bssid, rssi, lastseen_ts]\n",
    "            wifi.append(wifi_data)\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_BEACON':\n",
    "            ts = line_data[0]\n",
    "            uuid = line_data[2]\n",
    "            major = line_data[3]\n",
    "            minor = line_data[4]\n",
    "            rssi = line_data[6]\n",
    "            ibeacon_data = [ts, '_'.join([uuid, major, minor]), rssi]\n",
    "            ibeacon.append(ibeacon_data)\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_WAYPOINT':\n",
    "            waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n",
    "\n",
    "    acce = np.array(acce)\n",
    "    acce_uncali = np.array(acce_uncali)\n",
    "    gyro = np.array(gyro)\n",
    "    gyro_uncali = np.array(gyro_uncali)\n",
    "    magn = np.array(magn)\n",
    "    magn_uncali = np.array(magn_uncali)\n",
    "    ahrs = np.array(ahrs)\n",
    "    wifi = np.array(wifi)\n",
    "    ibeacon = np.array(ibeacon)\n",
    "    waypoint = np.array(waypoint)\n",
    "\n",
    "    return ReadData(acce, acce_uncali, gyro, gyro_uncali, magn, magn_uncali, ahrs, wifi, ibeacon, waypoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "experimental-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-artwork",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-ontario",
   "metadata": {},
   "source": [
    "set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preliminary-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "if not os.path.exists(DIR_SAVE_IGNORE):\n",
    "    os.makedirs(DIR_SAVE_IGNORE)\n",
    "if not os.path.exists(DIR_SAVE):\n",
    "    os.makedirs(DIR_SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-possession",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coordinated-filling",
   "metadata": {
    "executionInfo": {
     "elapsed": 12440,
     "status": "ok",
     "timestamp": 1616479159010,
     "user": {
      "displayName": "ユウキハヤカワ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjfXu9o7zmAYL24s4P8H7I0lvSgQUaSfrXi7qemvQ=s64",
      "userId": "00286900621093795459"
     },
     "user_tz": -540
    },
    "id": "g0ytMb1aSnv0"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(PATH_SUB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-legislation",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "primary-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = [val.split('_')[0] for val in sample_submission.site_path_timestamp]\n",
    "site_list = sorted(np.unique(site_list).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "lovely-appliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.08 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.05 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.22 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.06 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.12 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.03 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.03 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.10 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.11 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.13 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.09 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.08 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.02 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.08 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.05 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.02 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.09 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.04 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.11 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.08 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.19 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.12 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.22 Mb (33.3% reduction)\n",
      "Mem. usage decreased to  0.19 Mb (33.3% reduction)\n",
      "CPU times: user 10min 58s, sys: 8.44 s, total: 11min 7s\n",
      "Wall time: 11min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols = ['timestamp', 'x', 'y']\n",
    "# dict_site = {}\n",
    "mb = master_bar(site_list)\n",
    "for i_site, site in enumerate(mb):\n",
    "    path_route_list = sorted(glob(f'./../data_ignore/input/train/{site}/*/*'))\n",
    "#     df_data = pd.DataFrame(columns=cols)\n",
    "    df_list = []\n",
    "    for i_route, path_route in enumerate(progress_bar(path_route_list, parent=mb)):\n",
    "        floor = path_route.split('/')[-2]\n",
    "        data = read_data_file(path_route).waypoint\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df['floor'] = floor\n",
    "        df['route'] = path_route.split('/')[-1].split('.')[0]\n",
    "#         df_data = pd.concat([df_data, df], axis=0)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    save_path = f'{DIR_SAVE_IGNORE}nb{NB}_waypoint_{site}.csv'\n",
    "    df_data = pd.concat(df_list, axis=0)\n",
    "    df_data = reduce_mem_usage(df_data)\n",
    "    df_data.to_csv(save_path, index=False)\n",
    "#     if i_site == 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "future-skating",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
